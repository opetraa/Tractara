"""PDF íŒŒì‹± ëª¨ë“ˆ: í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ (Docling â†’ PyMuPDF â†’ Gemini Vision)."""
# src/tractara/parsing/pdf_parser.py
import io
import logging
import os
import re
import uuid
from collections import Counter
from dataclasses import dataclass, field
from pathlib import Path
from typing import Dict, List, Optional

# PyMuPDF ì„í¬íŠ¸
import pymupdf
from PIL import Image

from .section_classifier import (
    SectionClassifier,
    SectionFeatures,
    extract_section_label,
)

logger = logging.getLogger(__name__)


@dataclass
class BoundingBox:
    """PDF ì¢Œí‘œ ì •ë³´"""

    x0: float
    y0: float
    x1: float
    y1: float
    page: int

    def to_dict(self) -> Dict[str, float]:
        """ìì‹ ì„ Dictionaryë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        return {
            "x0": self.x0,
            "y0": self.y0,
            "x1": self.x1,
            "y1": self.y1,
            "page": self.page,
        }


@dataclass
class ParsedBlock:
    """ë‹¨ì¼ íŒŒì‹± ë¸”ë¡."""

    page: int
    block_type: str
    text: Optional[str] = None
    bbox: Optional[BoundingBox] = None
    table_data: Optional[Dict] = None
    equation_data: Optional[Dict] = None
    confidence: float = 1.0
    # ê³„ì¸µ êµ¬ì¡° í•„ë“œ
    level: int = 999  # 0: Title, 1: Section, 2+: Subsection, 999: Paragraph
    context_path: List[str] = field(default_factory=list)
    parent_id: Optional[str] = None
    block_id: Optional[str] = None
    # ì„¹ì…˜ ë©”íƒ€ë°ì´í„° (section_classifier ì—ì„œ ì¶”ì¶œ)
    section_label: Optional[str] = None  # ì˜ˆ: "1.2.3", "ì œ2ì¥"
    section_title: Optional[str] = None  # ë²ˆí˜¸ ì´í›„ ì œëª© í…ìŠ¤íŠ¸


@dataclass
class ParsedDocument:
    """íŒŒì‹±ëœ ì „ì²´ ë¬¸ì„œ ì»¨í…Œì´ë„ˆ."""

    source_path: str
    blocks: List[ParsedBlock]
    metadata: Optional[Dict] = None


class PyMuPDFParser:
    """
    í…ìŠ¤íŠ¸ ê¸°ë°˜ PDF íŒŒì„œ (PyMuPDF).

    íŒŒì´í”„ë¼ì¸:
      Phase 0 â€” ë¬¸ì„œ ë ˆë²¨ ì „ì²˜ë¦¬:
        - ë³¸ë¬¸ í°íŠ¸ í¬ê¸° ì¶”ì •
        - PDF ë¶ë§ˆí¬(Sê¸‰ íŒíŠ¸) ìˆ˜ì§‘
        - ToC í˜ì´ì§€ íŒŒì‹±(Sê¸‰ íŒíŠ¸) ì‹œë„
        - SectionClassifier ì´ˆê¸°í™”
      Phase 1 â€” ë¸”ë¡ ë£¨í”„:
        - ë¸”ë¡ íŠ¹ì§• ì¶”ì¶œ â†’ SectionClassifier.classify()
        - ìŠ¤íƒ ê¸°ë°˜ parent_id / context_path ì¶”ì 
    """

    def parse(self, pdf_path: Path) -> ParsedDocument:
        doc = pymupdf.open(pdf_path)
        blocks: List[ParsedBlock] = []

        # â”€â”€ Phase 0: ë¬¸ì„œ ë ˆë²¨ ì „ì²˜ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

        # ë³¸ë¬¸ í°íŠ¸ í¬ê¸° ì¶”ì • (ì „ì²´ span í°íŠ¸ í¬ê¸° ìµœë¹ˆê°’)
        font_sizes: List[float] = []
        for page in doc:
            for b in page.get_text("dict")["blocks"]:
                if b["type"] == 0:
                    for line in b["lines"]:
                        for span in line["spans"]:
                            if span["text"].strip():
                                font_sizes.append(round(span["size"], 1))

        body_font_size = (
            Counter(font_sizes).most_common(1)[0][0] if font_sizes else 10.0
        )
        logger.info("Detected body font size: %spt", body_font_size)

        # Sê¸‰ íŒíŠ¸ 1: PDF ë¶ë§ˆí¬
        pdf_bookmarks = doc.get_toc()  # [(level, title, page_no), ...]
        logger.info("PDF bookmarks found: %d", len(pdf_bookmarks))

        # Sê¸‰ íŒíŠ¸ 2: ToC í˜ì´ì§€ íŒŒì‹±
        toc_entries = self._extract_toc_entries(doc)
        logger.info("ToC entries parsed: %d", len(toc_entries))

        # ë¶„ë¥˜ê¸° ì´ˆê¸°í™”
        classifier = SectionClassifier(body_font_size, pdf_bookmarks, toc_entries)

        # â”€â”€ Phase 1: ë¸”ë¡ ë£¨í”„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

        # ìŠ¤íƒ: [{"level": int, "id": str, "title": str}]
        context_stack: List[Dict] = []

        for page_index, page in enumerate(doc):
            page_dict = page.get_text("dict")
            page_width = page.rect.width

            for block in page_dict.get("blocks", []):
                if block["type"] != 0:  # 0: text, 1: image
                    continue

                # ë¸”ë¡ íŠ¹ì§• ì¶”ì¶œ
                text_parts: List[str] = []
                max_font_size = 0.0
                is_bold = False
                font_name_counter: Counter = Counter()

                for line in block["lines"]:
                    for span in line["spans"]:
                        text_parts.append(span["text"])
                        if span["size"] > max_font_size:
                            max_font_size = span["size"]
                        if span["flags"] & 16:  # bit 4 = bold
                            is_bold = True
                        if span["text"].strip():
                            font_name_counter[span["font"]] += 1

                clean_text = " ".join(text_parts).strip()
                if not clean_text:
                    continue

                dominant_font = (
                    font_name_counter.most_common(1)[0][0] if font_name_counter else ""
                )
                bbox_x0, bbox_y0, bbox_x1, bbox_y1 = block["bbox"]

                # ë¶„ë¥˜ê¸° í˜¸ì¶œ
                features = SectionFeatures(
                    text=clean_text,
                    max_font_size=max_font_size,
                    is_bold=is_bold,
                    font_name=dominant_font,
                    page_width=page_width,
                    bbox_x0=bbox_x0,
                    bbox_x1=bbox_x1,
                )
                result = classifier.classify(features)

                # ìˆ˜ì‹ ê°ì§€ ë®ì–´ì“°ê¸° (íœ´ë¦¬ìŠ¤í‹±)
                equation_data = None
                if result.block_type == "paragraph" and self._is_equation(clean_text):
                    result.block_type = "equation"

                    # ìˆ˜ì‹ ë²ˆí˜¸ ì¶”ì¶œ ì‹œë„
                    eq_num_match = re.search(r"\((?P<num>\d+(\.\d+)*)\)$", clean_text)
                    eq_num = eq_num_match.group("num") if eq_num_match else None

                    equation_data = {
                        "latex": clean_text,  # ì›ì‹œ í…ìŠ¤íŠ¸ ë³´ì¡´
                        "equationNumber": eq_num if eq_num else "",
                    }

                # ìŠ¤íƒ ì¡°ì •: í˜„ì¬ ë ˆë²¨ë³´ë‹¤ ê¹Šê±°ë‚˜ ê°™ì€ ì´ì „ ì„¹ì…˜ ë‹«ê¸°
                level = result.level
                while context_stack and context_stack[-1]["level"] >= level:
                    context_stack.pop()

                # ë¶€ëª¨ ì—°ê²° ë° ì»¨í…ìŠ¤íŠ¸ ê²½ë¡œ ìˆ˜ì§‘
                parent_id = context_stack[-1]["id"] if context_stack else None
                current_context_path = [item["title"] for item in context_stack]
                block_id = str(uuid.uuid4())

                blocks.append(
                    ParsedBlock(
                        page=page_index + 1,
                        block_type=result.block_type,
                        text=clean_text,
                        bbox=BoundingBox(
                            x0=bbox_x0,
                            y0=bbox_y0,
                            x1=bbox_x1,
                            y1=bbox_y1,
                            page=page_index + 1,
                        ),
                        table_data=None,
                        equation_data=equation_data,
                        confidence=result.confidence,
                        level=level,
                        context_path=current_context_path,
                        parent_id=parent_id,
                        block_id=block_id,
                        section_label=result.section_label,
                        section_title=result.section_title,
                    )
                )

                # ì„¹ì…˜ë§Œ ìŠ¤íƒì— í‘¸ì‹œ (paragraphë‚˜ equationì€ ë¶€ëª¨ê°€ ë  ìˆ˜ ì—†ìŒ)
                if level < 999:
                    context_stack.append(
                        {
                            "level": level,
                            "id": block_id,
                            "title": clean_text,
                        }
                    )

        doc.close()

        # í›„ì²˜ë¦¬ 1: ë¬¸ë§¥ ê¸°ë°˜ ë¶„í•  (ì¸ë¼ì¸ ìˆ˜ì‹ ì¶”ì¶œ)
        blocks = _split_inline_equations(blocks)

        # í›„ì²˜ë¦¬ 2: ë¬¸ë§¥ ê¸°ë°˜ ìˆ˜ì‹ íƒì§€ ë° ì¬ë¶„ë¥˜ (ë‹¨ë… ë¸”ë¡ ëŒ€ìƒ)
        blocks = _reclassify_equations(blocks)

        return ParsedDocument(
            source_path=str(pdf_path),
            blocks=blocks,
            metadata={"parser": "pymupdf_section_classifier", "version": "3.0.0"},
        )

    # â”€â”€ ë‚´ë¶€ í—¬í¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _extract_toc_entries(self, doc) -> List[Dict]:
        """
        ToC í˜ì´ì§€ë¥¼ íƒì§€í•˜ê³  ì„¹ì…˜ ì—”íŠ¸ë¦¬ë¥¼ íŒŒì‹±í•œë‹¤.

        íƒì§€ ì „ëµ:
          - ì²« 15í˜ì´ì§€ì—ì„œ "contents" / "ëª©ì°¨" / "table of contents" í‚¤ì›Œë“œ ê²€ìƒ‰
          - ë°œê²¬ëœ í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ ë¼ì¸ì—ì„œ ì ì„ +í˜ì´ì§€ë²ˆí˜¸ ì œê±° í›„ ì„¹ì…˜ ë¼ë²¨ ì¶”ì¶œ

        ë°˜í™˜: [{"label": "1.2", "title": "Background"}, ...]
        """
        entries: List[Dict] = []

        for page_index, page in enumerate(doc):
            if page_index > 15:
                break

            page_text = page.get_text().strip()
            first_300 = page_text[:300].lower()

            toc_keywords = ("contents", "ëª©ì°¨", "table of contents")
            if not any(kw in first_300 for kw in toc_keywords):
                continue

            # ToC í˜ì´ì§€ ë°œê²¬ â†’ ë¼ì¸ë³„ íŒŒì‹±
            for line in page_text.split("\n"):
                line = line.strip()
                if len(line) < 3:
                    continue

                # ì ì„  ë° ë í˜ì´ì§€ ë²ˆí˜¸ ì œê±°
                # ì˜ˆ: "1.2 Background ............. 45" â†’ "1.2 Background"
                cleaned = re.sub(r"[.\s]{3,}\d+\s*$", "", line).strip()
                cleaned = re.sub(r"\.{3,}", "", cleaned).strip()

                if len(cleaned) < 3:
                    continue

                label, title = extract_section_label(cleaned)
                if label:
                    entries.append({"label": label, "title": title or cleaned})

            # ì²« ë²ˆì§¸ ToC í˜ì´ì§€ë§Œ ì²˜ë¦¬
            break

        return entries

    def _is_equation(self, text: str) -> bool:
        """ìˆ˜ì‹ ì—¬ë¶€ íœ´ë¦¬ìŠ¤í‹± íƒì§€"""
        # ëì´ ê´„í˜¸ ë²ˆí˜¸ë¡œ ëë‚˜ëŠ” ê²½ìš°: "E = mc^2 (1.1)"
        if re.search(r"\(\d+(\.\d+)*\)$", text.strip()):
            return True

        # ìˆ˜í•™ ê¸°í˜¸ ë¹„ìœ¨ í™•ì¸
        math_symbols = set("âˆ‘âˆ«âˆšÂ±Î±Î²Î³Î´ÎµÎ¶Î·Î¸Î¹ÎºÎ»Î¼Î½Î¾Î¿Ï€ÏÏƒÏ„Ï…Ï†Ï‡ÏˆÏ‰Î”Î“Î˜Î›ÎÎ Î£Î¦Î¨Î©=+-*/<>â‰¤â‰¥â‰ˆâ‰ ")
        symbol_count = sum(1 for char in text if char in math_symbols)
        # 3ê°œ ì´ìƒì˜ ìˆ˜ì‹ ê¸°í˜¸ê°€ ìˆê±°ë‚˜ ì „ì²´ ê¸¸ì´ ëŒ€ë¹„ ê¸°í˜¸ ë¹„ìœ¨ì´ 10% ì´ìƒì´ë©´ ìˆ˜ì‹ìœ¼ë¡œ ê°„ì£¼ (ë°©ì •ì‹ ë“±)
        if len(text) > 3 and (symbol_count >= 3 or (symbol_count / len(text)) > 0.1):
            return True

        return False


class DoclingParser:
    """
    ë©”ì¸ íŒŒì„œ: Docling ê¸°ë°˜ (í‘œ + ë ˆì´ì•„ì›ƒ + ê³„ì¸µ êµ¬ì¡° ì „ë¬¸).

    Doclingì€ ìì²´ì ìœ¼ë¡œ ê³„ì¸µ êµ¬ì¡°ë¥¼ ì œê³µí•˜ë¯€ë¡œ SectionClassifierë¥¼ ìš°íšŒí•œë‹¤.
    section/title ë¸”ë¡ì— í•œí•´ extract_section_label()ë¡œ sectionLabel/sectionTitleì„ ì¶”ì¶œí•œë‹¤.
    """

    def __init__(self):
        try:
            import torch
            from docling.datamodel.base_models import InputFormat
            from docling.datamodel.pipeline_options import (
                AcceleratorDevice,
                AcceleratorOptions,
                PdfPipelineOptions,
            )
            from docling.document_converter import DocumentConverter, PdfFormatOption

            if torch.cuda.is_available():
                logger.info(
                    "ğŸš€ GPU detected (CUDA: %s). Using CUDA for Docling.",
                    torch.cuda.get_device_name(0),
                )
                device = AcceleratorDevice.CUDA
            elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
                logger.info("ğŸš€ GPU detected (Apple MPS). Using MPS for Docling.")
                device = getattr(AcceleratorDevice, "MPS", AcceleratorDevice.CPU)
            else:
                logger.info(
                    "â„¹ï¸ GPU not detected (CUDA/MPS unavailable). Using CPU for Docling."
                )
                device = AcceleratorDevice.CPU

            pipeline_options = PdfPipelineOptions()
            pipeline_options.accelerator_options = AcceleratorOptions(
                num_threads=4, device=device
            )

            self.converter = DocumentConverter(
                format_options={
                    InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
                }
            )

            try:
                pass  # CV2 Optional dependency check removed for pylint compliance

                self.converter.format_to_options[
                    InputFormat.PDF
                ].pipeline_options.do_table_structure = True
            except ImportError:
                logger.warning("OpenCV(cv2) ì—†ìŒ. í‘œ êµ¬ì¡° ì¶”ì¶œ ê¸°ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                self.converter.format_to_options[
                    InputFormat.PDF
                ].pipeline_options.do_table_structure = False

        except ImportError as e:
            raise ImportError(f"Docling ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {e}") from e

    def parse(
        self, pdf_path: Path
    ) -> ParsedDocument:  # pylint: disable=too-many-locals
        """Docling ê¸°ë°˜ PDF íŒŒì‹±."""
        result = self.converter.convert(pdf_path)
        doc = result.document
        blocks: List[ParsedBlock] = []

        context_stack: List[Dict] = []

        for item, level in doc.iterate_items():
            label = str(getattr(item, "label", "")).lower()
            text = getattr(item, "text", "").strip()

            if not text and "table" not in label:
                continue

            # íƒ€ì… ë§¤í•‘
            block_type = "paragraph"
            if "title" in label:
                block_type = "title"
            elif "header" in label:
                block_type = "section"
            elif "table" in label:
                block_type = "table"
            elif "formula" in label or "equation" in label:
                block_type = "equation"
            elif "list" in label:
                block_type = "list"

            # ìŠ¤íƒ ì¡°ì • (Docling levelì´ Noneì¸ ë³¸ë¬¸ì€ ìŠ¤íƒ ìœ ì§€)
            if block_type in ["title", "section"] and level is not None:
                while context_stack and context_stack[-1]["level"] >= level:
                    context_stack.pop()

            parent_id = context_stack[-1]["id"] if context_stack else None
            current_context_path = [item["title"] for item in context_stack]
            block_id = str(uuid.uuid4())

            # sectionLabel / sectionTitle ì¶”ì¶œ (Doclingì€ classifier ìš°íšŒ)
            sec_label, sec_title = None, None
            if block_type in ["title", "section"]:
                sec_label, sec_title = extract_section_label(text)
                if sec_title is None:
                    sec_title = text

            bbox = self._extract_bbox(item)

            parsed_block = ParsedBlock(
                page=item.prov[0].page_no if hasattr(item, "prov") and item.prov else 1,
                block_type=block_type,
                text=text,
                bbox=bbox,
                confidence=1.0,
                level=level if level is not None else 999,
                context_path=current_context_path,
                parent_id=parent_id,
                block_id=block_id,
                section_label=sec_label,
                section_title=sec_title,
            )

            if block_type == "table" and hasattr(item, "export_to_dataframe"):
                try:
                    df = item.export_to_dataframe()
                    parsed_block.table_data = {
                        "headers": [str(h) for h in df.columns.tolist()],
                        "rows": [[str(c) for c in row] for row in df.values.tolist()],
                    }
                    parsed_block.text = df.to_markdown(index=False)
                except (AttributeError, ValueError, KeyError):
                    pass
            elif block_type == "equation":
                parsed_block.equation_data = {"latex": text}

            blocks.append(parsed_block)

            if block_type in ["title", "section"] and level is not None:
                context_stack.append(
                    {
                        "level": level,
                        "id": block_id,
                        "title": text,
                    }
                )

        # í›„ì²˜ë¦¬ 1: ë¬¸ë§¥ ê¸°ë°˜ ë¶„í•  (ì¸ë¼ì¸ ìˆ˜ì‹ ì¶”ì¶œ)
        blocks = _split_inline_equations(blocks)

        # í›„ì²˜ë¦¬ 2: ë¬¸ë§¥ ê¸°ë°˜ ìˆ˜ì‹ íƒì§€ ë° ì¬ë¶„ë¥˜ (ë‹¨ë… ë¸”ë¡ ëŒ€ìƒ)
        blocks = _reclassify_equations(blocks)

        return ParsedDocument(
            source_path=str(pdf_path),
            blocks=blocks,
            metadata={"parser": "docling", "version": "2.0.0"},
        )

    def _extract_bbox(self, item) -> Optional[BoundingBox]:
        """Docling ì•„ì´í…œì—ì„œ BoundingBox ì¶”ì¶œ."""
        if hasattr(item, "prov") and item.prov:
            p = item.prov[0]
            b = p.bbox
            return BoundingBox(
                x0=getattr(b, "l", 0),
                y0=getattr(b, "b", 0),
                x1=getattr(b, "r", 0),
                y1=getattr(b, "t", 0),
                page=p.page_no,
            )
        return None


class GeminiVisionParser:
    """
    ë°±ì—… íŒŒì„œ: ìŠ¤ìº”ëœ ë¬¸ì„œë‚˜ ë³µì¡í•œ í‘œ ì²˜ë¦¬ë¥¼ ìœ„í•œ VLM (Vision-Language Model).
    gemini-3-flash-previewë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì—ì„œ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¥¼ ì¶”ì¶œ.
    """

    def __init__(self, api_key: Optional[str] = None):
        import google.generativeai as genai  # type: ignore

        self.api_key = (
            api_key or os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
        )
        if not self.api_key:
            raise ValueError("Gemini API Key is missing for Vision Parser.")

        genai.configure(api_key=self.api_key)
        self.model_name = "gemini-3-flash-preview"

    def parse(self, pdf_path: Path) -> ParsedDocument:
        """PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜ í›„ Geminiì—ê²Œ êµ¬ì¡°í™” ìš”ì²­."""
        doc = pymupdf.open(pdf_path)
        blocks: List[ParsedBlock] = []

        for page_index, page in enumerate(doc):
            if page_index >= 3:
                break

            pix = page.get_pixmap(dpi=150)
            img_data = pix.tobytes("png")
            image = Image.open(io.BytesIO(img_data))

            prompt = (
                "Extract all text from this page. Return raw text. "
                "For equations or mathematical formulas, "
                "extract them strictly in LaTeX format."
            )
            import google.generativeai as genai

            response = genai.GenerativeModel(self.model_name).generate_content(
                contents=[prompt, image],
            )

            blocks.append(
                ParsedBlock(
                    page=page_index + 1,
                    block_type="paragraph",
                    text=response.text,
                    confidence=0.8,
                )
            )

        doc.close()

        return ParsedDocument(
            source_path=str(pdf_path),
            blocks=blocks,
            metadata={"parser": "gemini_vision", "version": "1.0.0"},
        )


def _ocr_equation_region(
    page: "pymupdf.Page",  # type: ignore[name-defined]
    x0: float,
    y0: float,
    x1: float,
    y1: float,
    padding: int = 10,
) -> Optional[str]:
    """ìˆ˜ì‹ BBox ì˜ì—­ì„ ì´ë¯¸ì§€ë¡œ í¬ë¡­ í›„ Gemini Visionìœ¼ë¡œ LaTeXë¥¼ ì¶”ì¶œí•œë‹¤.

    API í‚¤ê°€ ì—†ê±°ë‚˜ í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ Noneì„ ë°˜í™˜í•˜ì—¬ í˜¸ì¶œìê°€ fallback í•  ìˆ˜ ìˆê²Œ í•œë‹¤.
    """
    api_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
    if not api_key:
        return None

    try:
        # 1. BBox â†’ í¬ë¡­ ì´ë¯¸ì§€ (íŒ¨ë”© ì¶”ê°€ë¡œ ì˜ë¦¼ ë°©ì§€)
        clip = pymupdf.Rect(
            x0 - padding,
            y0 - padding,
            x1 + padding,
            y1 + padding,
        )
        pix = page.get_pixmap(dpi=300, clip=clip)
        img_bytes = pix.tobytes("png")

        # 2. Gemini Vision í˜¸ì¶œ
        import google.generativeai as genai  # pylint: disable=import-outside-toplevel

        genai.configure(api_key=api_key)
        model = genai.GenerativeModel("gemini-3-flash-preview")

        prompt = (
            "This image contains a single mathematical equation. "
            "Return ONLY the equation in LaTeX format, without equation numbers, "
            "without $$ delimiters, without any explanation. "
            "Example: F_{en} = \\exp(0.935 - T^* \\dot{\\varepsilon}^* O^*)"
        )

        image = Image.open(io.BytesIO(img_bytes))
        response = model.generate_content(contents=[prompt, image])
        latex = response.text.strip()

        # ë¹ˆ ì‘ë‹µì´ë‚˜ ì˜¤ë¥˜ ë©”ì‹œì§€ í•„í„°
        if not latex or len(latex) < 3:
            return None

        logger.info("Vision OCR for equation: %s", latex)
        return latex

    except Exception as exc:  # pylint: disable=broad-except
        logger.warning("Vision OCR failed, falling back to PyMuPDF text: %s", exc)
        return None


def _supplement_missing_equations(
    doc_path: Path, docling_blocks: List[ParsedBlock]
) -> List[ParsedBlock]:
    """
    Doclingì´ í†µì§¸ë¡œ í…ìŠ¤íŠ¸ë¥¼ ëˆ„ë½í•œ ì˜ì—­ì— ëŒ€í•´ PyMuPDFë¡œ ê°€ë³ê²Œ ìŠ¤ìº”í•˜ì—¬
    ìˆ˜ì‹ íŒ¨í„´ì´ ìˆëŠ” ë¸”ë¡ì„ ë³´ì¶©í•œë‹¤.
    Gemini Vision API í‚¤ê°€ ìˆìœ¼ë©´ í¬ë¡­ OCRë¡œ ì •í™•í•œ LaTeXë¥¼ ì·¨ë“í•œë‹¤.
    """
    try:
        doc = pymupdf.open(doc_path)
    except Exception as e:
        logger.warning("Surgical supplement failed to open PDF: %s", e)
        return docling_blocks

    supplemented_blocks = list(docling_blocks)

    # ì˜ˆ: "(12)" ê°™ì€ ê´„í˜¸í˜• ìˆ˜ì‹ ë²ˆí˜¸ë¡œ ëë‚˜ëŠ” íŒ¨í„´
    eq_num_pattern = re.compile(r"\(\s*(?P<num>\d+(\.\d+)*[a-zA-Z]?)\s*\)\s*$")
    # ìˆ˜í•™ í•¨ìˆ˜ëª… íŒ¨í„´ (= ê¸°í˜¸ê°€ í°íŠ¸ ë””ì½”ë”© ì˜¤ë¥˜ë¡œ ì†Œì‹¤ëœ ê²½ìš° ë³´ì¡° íƒì§€ìš©)
    math_func_pattern = re.compile(r"\b(exp|ln|log|sin|cos|tan|sqrt)\b", re.IGNORECASE)

    # í˜ì´ì§€ë³„ë¡œ docling blockë“¤ì˜ bboxë¥¼ O(N) ë¹„êµë¥¼ ìœ„í•´ ë¯¸ë¦¬ êµ¬ì„±
    from collections import defaultdict

    docling_bboxes_by_page = defaultdict(list)

    # PDF í˜ì´ì§€ ë†’ì´(Height)ë¥¼ êµ¬í•´ì„œ Yì¢Œí‘œë¥¼ í†µì¼í•˜ê¸° ìœ„í•´ ë¯¸ë¦¬ í•œ ë²ˆ ìŠ¤ìº”
    page_heights = {}
    for p_idx, p in enumerate(doc):
        page_heights[p_idx + 1] = p.rect.height

    for b in docling_blocks:
        if b.bbox and b.text and len(b.text.strip()) >= 5:
            ph = page_heights.get(b.bbox.page, 800)
            dy0 = ph - b.bbox.y1
            dy1 = ph - b.bbox.y0
            if dy0 < 0 or dy1 < 0:
                dy0, dy1 = b.bbox.y0, b.bbox.y1

            docling_bboxes_by_page[b.bbox.page].append(
                {
                    "x0": b.bbox.x0 - 5,
                    "y0": dy0 - 5,
                    "x1": b.bbox.x1 + 5,
                    "y1": dy1 + 5,
                    "text": b.text,
                }
            )

    added_count = 0
    for page_index, page in enumerate(doc):
        page_num = page_index + 1
        page_dict = page.get_text("dict")

        for block in page_dict.get("blocks", []):
            if block["type"] != 0:  # text block
                continue

            text_parts = []
            for line in block["lines"]:
                for span in line["spans"]:
                    text_parts.append(span["text"])

            text = " ".join(text_parts).strip()
            if not text or len(text) < 5:
                continue

            # ìˆ˜ì‹ ë²ˆí˜¸ íŒ¨í„´ì´ ëì— ìˆì–´ì•¼ í•¨ (í•„ìˆ˜)
            eq_num_match = eq_num_pattern.search(text)
            if not eq_num_match:
                continue

            # '=' ê¸°í˜¸ ë˜ëŠ” ìˆ˜í•™ í•¨ìˆ˜ëª…ì´ ìˆì–´ì•¼ ìˆ˜ì‹ìœ¼ë¡œ ì¸ì •
            has_equals = "=" in text
            has_math_func = bool(math_func_pattern.search(text))
            if not has_equals and not has_math_func:
                continue

            bx0, by0, bx1, by1 = block["bbox"]

            # ê²¹ì¹¨ í™•ì¸ (Doclingì´ ì´ë¯¸ ì¡ì•˜ë‹¤ë©´ íŒ¨ìŠ¤)
            is_overlap = False
            for db in docling_bboxes_by_page[page_num]:
                dx0, dy0, dx1, dy1 = db["x0"], db["y0"], db["x1"], db["y1"]
                if not (bx1 < dx0 or bx0 > dx1 or by1 < dy0 or by0 > dy1):
                    is_overlap = True
                    break

            if not is_overlap:
                eq_num = eq_num_match.group("num").strip()

                # Gemini Visionìœ¼ë¡œ ì •í™•í•œ LaTeX ì·¨ë“ ì‹œë„
                vision_latex = _ocr_equation_region(page, bx0, by0, bx1, by1)

                if vision_latex:
                    latex_text = vision_latex
                    display_text = vision_latex
                else:
                    # fallback: PyMuPDF í…ìŠ¤íŠ¸ì—ì„œ ìˆ˜ì‹ ë²ˆí˜¸ ì œê±° í›„ ì‚¬ìš©
                    latex_text = eq_num_pattern.sub("", text).strip()
                    latex_text = re.sub(r"\s{2,}", " ", latex_text)
                    display_text = text

                new_block = ParsedBlock(
                    page=page_num,
                    block_type="equation",
                    text=display_text,
                    equation_data={"latex": latex_text, "equationNumber": eq_num},
                    bbox=BoundingBox(x0=bx0, y0=by0, x1=bx1, y1=by1, page=page_num),
                    level=999,
                    block_id=str(uuid.uuid4()),
                )
                supplemented_blocks.append(new_block)
                added_count += 1

    doc.close()

    if added_count > 0:
        logger.info(
            "Surgically supplemented %d missing equations using PyMuPDF.", added_count
        )
        # í˜ì´ì§€ì™€ y ì¢Œí‘œ ìˆœìœ¼ë¡œ ì¬ì •ë ¬
        supplemented_blocks.sort(key=lambda b: (b.page, b.bbox.y0 if b.bbox else 0))

    return supplemented_blocks


def _split_inline_equations(blocks: List[ParsedBlock]) -> List[ParsedBlock]:
    """
    ê¸´ ë¬¸ë‹¨(paragraph) ë‚´ì— ë¼ì–´ìˆëŠ” ìˆ˜ì‹ì„ ë¶„ë¦¬í•˜ì—¬ ë³„ë„ equation ë¸”ë¡ìœ¼ë¡œ ì¶”ì¶œí•œë‹¤.
    ì•µì»¤: '(ìˆ«ì)' í˜¹ì€ '(ìˆ«ì.ìˆ«ì)' í˜•íƒœì˜ ìˆ˜ì‹ ë²ˆí˜¸ê°€ ë¬¸ë‹¨ ì¤‘ê°„/ëì— ë“±ì¥í•˜ê³ ,
          ê·¸ ì•ì— ìˆ˜ì‹ ê¸°í˜¸(=)ê°€ ì¡´ì¬í•˜ëŠ” íŒ¨í„´ì„ ì°¾ëŠ”ë‹¤.
    """
    new_blocks: List[ParsedBlock] = []

    # ì˜ˆ: "(12)", "(1.1)", "(13a)" ë“± ìˆ˜ì‹ ë²ˆí˜¸ íŒ¨í„´ (ë‹¨ì–´ ê²½ê³„ í™•ì¸)
    # í…ìŠ¤íŠ¸ ë‚´ì— ì¤„ë°”ê¿ˆì´ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ re.DOTALL ì ìš©
    eq_num_pattern = re.compile(
        r"(?P<eq_text>.+?)\(\s*(?P<num>\d+(\.\d+)*[a-zA-Z]?)\s*\)", re.DOTALL
    )

    for block in blocks:
        if block.block_type != "paragraph" or not block.text or len(block.text) < 30:
            new_blocks.append(block)
            continue

        text = block.text
        # = ê¸°í˜¸ê°€ ì—†ìœ¼ë©´ ì¸ë¼ì¸ ìˆ˜ì‹ìœ¼ë¡œ ì·¨ê¸‰ ì•ˆ í•¨
        if "=" not in text:
            new_blocks.append(block)
            continue

        parts_handled = False
        remaining_text = text
        block_splits: List[ParsedBlock] = []

        while True:
            match = eq_num_pattern.search(remaining_text)
            if not match:
                break

            candidate_text = match.group("eq_text").strip()
            eq_num = match.group("num")

            # ìˆ˜ì‹ ê¸°í˜¸ê°€ ì—†ìœ¼ë©´ ì¼ë°˜ ì°¸ì¡°ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ íŒ¨ìŠ¤
            if "=" not in candidate_text:
                # í˜„ì¬ ë§¤ì¹˜ì˜ ë ìœ„ì¹˜ ë‹¤ìŒë¶€í„° ë‚¨ì€ í…ìŠ¤íŠ¸ ì¬íƒìƒ‰
                remaining_text = remaining_text[match.end() :].strip()
                continue

            # ì—­ë°©í–¥ìœ¼ë¡œ ë„ì… í‚¤ì›Œë“œ ì°¾ê¸° (ì˜ˆ: given by)
            # ê°€ì¥ ê°€ê¹Œìš´ ë„ì… í‚¤ì›Œë“œë‚˜ ë¬¸ì¥ ë ë¶€í˜¸ ì´í›„ë¥¼ ìˆ˜ì‹ì˜ ì‹œì‘ìœ¼ë¡œ ê°„ì£¼
            best_start_idx = 0

            # 1. ë„ì… í‚¤ì›Œë“œ ì°¾ê¸°
            for kw in ["where", "given by", "as follows", "defined as", "is:", "is "]:
                # ë§ˆì§€ë§‰ ë“±ì¥ì„ ì°¾ê¸°
                idx = candidate_text.lower().rfind(kw)
                if idx != -1:
                    # ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œì˜ ì¸ë±ìŠ¤ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìë¦„
                    best_start_idx = max(best_start_idx, idx + len(kw))

            # 2. ë¬¸ì¥ ë ë¶€í˜¸ ì°¾ê¸° (ë„ì… í‚¤ì›Œë“œê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ë” ê°€ê¹Œìš´ ë¶€í’ˆì´ ìˆì„ ë•Œ)
            for punc in [". ", ": ", "; ", ".\n", ":\n", ";\n"]:
                idx = candidate_text.rfind(punc)
                if idx != -1:
                    best_start_idx = max(best_start_idx, idx + len(punc))

            # ë§Œì•½ ì°¾ì§€ ëª»í–ˆë‹¤ë©´ ì „ì²´ë¥¼ ìˆ˜ì‹ìœ¼ë¡œ ë³¼ì§€ íŒë‹¨.
            # í•˜ì§€ë§Œ = ê¸°í˜¸ê°€ ì•„ì£¼ ì•ìª½ì— ìˆë‹¤ë©´ ì „ì²´ë¥¼ ìˆ˜ì‹ìœ¼ë¡œ ì‚¼ì„ ìˆ˜ ìˆìŒ.
            if best_start_idx == 0:
                eq_start_idx = candidate_text.find("=")
                if eq_start_idx > 80:  # ë„ˆë¬´ ë©€ë©´ ì˜¤íƒì§€(ê¸€ì´ ê¸¸ ë•Œ) ë°©ì§€, í•˜ì§€ë§Œ ì—¬ìœ ë¥¼ ì¢€ ë‘ 
                    # ìˆ˜ì‹ ë¶„í•  í¬ê¸°í•˜ê³  ìŠ¤í‚µ
                    remaining_text = remaining_text[match.end() :].strip()
                    continue

            prefix = remaining_text[: match.start() + best_start_idx].strip()
            equation_body = candidate_text[best_start_idx:].strip()

            # ìˆ˜ì‹ ë¶„í•  ì¡°ê±´: ìµœì†Œí•œì˜ ê¸¸ì´ì™€ '=' í¬í•¨ ê²€ì¦
            if len(equation_body) < 3 or "=" not in equation_body:
                remaining_text = remaining_text[match.end() :].strip()
                continue

            # prefixê°€ ìˆìœ¼ë©´ ì•ë¶€ë¶„ paragraph ìƒì„±
            if prefix:
                prefix_block = ParsedBlock(
                    page=block.page,
                    block_type="paragraph",
                    text=prefix,
                    bbox=block.bbox,
                    level=block.level,
                    context_path=block.context_path,
                    parent_id=block.parent_id,
                    block_id=str(uuid.uuid4()),
                )
                block_splits.append(prefix_block)

            # equation ë¸”ë¡ ìƒì„±
            eq_block = ParsedBlock(
                page=block.page,
                block_type="equation",
                text=equation_body,
                equation_data={"latex": equation_body, "equationNumber": eq_num},
                bbox=block.bbox,
                level=block.level,
                context_path=block.context_path,
                parent_id=block.parent_id,
                block_id=str(uuid.uuid4()),
                section_label=block.section_label,
                section_title=block.section_title,
            )
            block_splits.append(eq_block)

            remaining_text = remaining_text[match.end() :].strip()
            parts_handled = True

        if parts_handled:
            # ë‚¨ì€ ë’·ë¶€ë¶„ í…ìŠ¤íŠ¸
            if remaining_text:
                suffix_block = ParsedBlock(
                    page=block.page,
                    block_type="paragraph",
                    text=remaining_text,
                    bbox=block.bbox,
                    level=block.level,
                    context_path=block.context_path,
                    parent_id=block.parent_id,
                    block_id=str(uuid.uuid4()),
                )
                block_splits.append(suffix_block)

            new_blocks.extend(block_splits)
        else:
            new_blocks.append(block)

    return new_blocks


def _reclassify_equations(blocks: List[ParsedBlock]) -> List[ParsedBlock]:
    """
    íŒŒì‹±ëœ ë¸”ë¡ ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆœíšŒí•˜ë©° ì¢Œìš° ë¬¸ë§¥(Context)ì„ í‰ê°€í•˜ì—¬ ìˆ˜ì‹ì„ íƒì§€/ì¬ë¶„ë¥˜í•œë‹¤.
    ê¸°ì¡´ paragraph ë¸”ë¡ ì¤‘ì—ì„œ ìˆ˜ì‹ì¼ ê°€ëŠ¥ì„±ì´ ë†’ì€ ê²ƒì„ íŒë‹¨.
    """
    for i, block in enumerate(blocks):
        if block.block_type != "paragraph" or not block.text:
            continue

        text = block.text.strip()
        if not text:
            continue

        score = 0

        # S1: ì•„ì£¼ ì§§ì€ í…ìŠ¤íŠ¸ ê¸¸ì´ (ì¼ë°˜ ë¬¸ë‹¨ì€ ë³´í†µ ê¸¸ìŒ)
        if len(text) < 100:
            score += 1

        # S2: ê´„í˜¸í˜• ìˆ˜ì‹ ë²ˆí˜¸ë¡œ ëë‚˜ëŠ” íŒ¨í„´
        eq_num = None
        eq_num_match = re.search(r"\((?P<num>\d+(\.\d+)*[a-zA-Z]?)\)\s*$", text)
        if eq_num_match:
            eq_num = eq_num_match.group("num")
            score += 2  # ê°•í•œ ì‹œê·¸ë„

        # ì£¼ë³€ ë¸”ë¡ ì»¨í…ìŠ¤íŠ¸
        prev_block_text = blocks[i - 1].text if i > 0 else None
        next_block_text = blocks[i + 1].text if i < len(blocks) - 1 else None
        prev_text = prev_block_text.strip().lower() if prev_block_text else ""
        next_text = next_block_text.strip().lower() if next_block_text else ""

        # S3: ì• ë¬¸ë‹¨ ë í‚¤ì›Œë“œ
        prev_keywords = (
            "where",
            "given by",
            "defined as",
            "as follows",
            "expressed as",
            "is",
            "equation",
            "equation:",
            "ì‹",
            "ë‹¤ìŒê³¼ ê°™ë‹¤",
        )
        if any(prev_text.endswith(kw) for kw in prev_keywords):
            score += 2

        # S4: ë’¤ ë¬¸ë‹¨ ì‹œì‘ í‚¤ì›Œë“œ
        next_keywords = ("where", "from", "in which", "here", "ì—¬ê¸°ì„œ")
        if any(next_text.startswith(kw) for kw in next_keywords):
            score += 1

        # S5: ì£¼ë³€ ë¸”ë¡ ìˆ˜ì‹ ì§€ì¹­ ë‹¨ì–´ (ë³¸ë¬¸ì´ ì•„ë‹ˆë¼ ë‚´ë¶€ì— ìˆì„ ë•Œë§Œ)
        surrounding_text = prev_text + " " + next_text
        if re.search(r"\b(eq\.|eqs\.|equation|ìˆ˜ì‹)\b", surrounding_text):
            score += 1

        # S6: íŠ¹ìˆ˜ ìˆ˜í•™ ê¸°í˜¸ (ìœ ë‹ˆì½”ë“œ ë° ì—°ì‚°ì)
        # í•˜ì´í”ˆ(-)ì€ ì¼ë°˜ í…ìŠ¤íŠ¸ì—ì„œë„ ë§ì´ ì“°ì´ë¯€ë¡œ, ë‹¨ë… í•˜ì´í”ˆë§Œ ìˆëŠ” ê²½ìš°ëŠ” ê¸°í˜¸ì—ì„œ ì œì™¸í•˜ê±°ë‚˜ ê°€ì¤‘ì¹˜ë¥¼ ë‚®ì¶¤
        math_symbols = set("âˆ‘âˆ«âˆšÂ±Î±Î²Î³Î´ÎµÎ¶Î·Î¸Î¹ÎºÎ»Î¼Î½Î¾Î¿Ï€ÏÏƒÏ„Ï…Ï†Ï‡ÏˆÏ‰Î”Î“Î˜Î›ÎÎ Î£Î¦Î¨Î©=+-*/<>â‰¤â‰¥â‰ˆâ‰ ")
        symbol_count = sum(1 for char in text if char in math_symbols)
        # = ê¸°í˜¸ê°€ ìˆê±°ë‚˜ ì „ì²´ ê¸°í˜¸ê°€ 2ê°œ ì´ìƒì¼ ë•Œ
        if "=" in text or symbol_count >= 2:
            score += 2

        # S7 (Penalty) : ì¼ë°˜ ë¬¸ì¥í˜• í…ìŠ¤íŠ¸ ë° ì˜¤íƒì§€(ì´ë©”ì¼, ì „í™”ë²ˆí˜¸ ë“±) ë°©ì§€
        # 1. í…ìŠ¤íŠ¸ ìì²´ê°€ "given by", "where" ë“±ìœ¼ë¡œ ëë‚˜ëŠ” ê±´ ë³´í†µ ë„ì… ë¬¸ì¥
        if any(text.lower().endswith(kw) for kw in prev_keywords):
            score -= 3
        # 2. ë„ˆë¬´ ê¸´ ë¬¸ì¥ì€ ìˆ˜ì‹ ì•„ë‹˜ (íŠ¹ìˆ˜ê¸°í˜¸ê°€ ë§ì§€ ì•Šì€ í•œ)
        if len(text) > 200 and symbol_count < 3:
            score -= 3
        # 3. ì´ë©”ì¼ ì£¼ì†Œ í˜•íƒœì¸ ê²½ìš°
        if "@" in text and re.search(r"[\w\.-]+@[\w\.-]+", text):
            score -= 5
        # 4. ì „í™”ë²ˆí˜¸/íŒ©ìŠ¤ ë²ˆí˜¸ í˜•íƒœ (ì˜ˆ: Facsimile: 301-415-2289)
        if re.search(r"\d{2,3}-\d{3,4}-\d{4}", text) and "=" not in text:
            score -= 5

        # ìµœì¢… í‰ê°€: 3ì  ì´ìƒì´ë©´ ìˆ˜ì‹ìœ¼ë¡œ íŒë‹¨
        if score >= 3:
            block.block_type = "equation"
            block.equation_data = {
                "latex": text,  # í–¥í›„ ìˆ˜ì‹ ì •ì œ ëª¨ë¸ì´ ë¶™ëŠ”ë‹¤ë©´ ì—¬ê¸°ì„œ ì²˜ë¦¬
                "equationNumber": eq_num if eq_num else "",
            }

    return blocks


def parse_pdf(path: Path) -> ParsedDocument:
    """
    í•˜ì´ë¸Œë¦¬ë“œ íŒŒì‹± ì „ëµ: Docling (ìµœìš°ì„ ) â†’ PyMuPDF â†’ Gemini Vision (ìŠ¤ìº”ë³¸)

    1. Docling: í‘œ, ë ˆì´ì•„ì›ƒ, ê³„ì¸µ êµ¬ì¡° ì™„ë²½ ì§€ì› (SectionClassifier ìš°íšŒ)
    2. PyMuPDF: ì•ˆì •ì  í…ìŠ¤íŠ¸ ì¶”ì¶œ + SectionClassifier ì ìš©
    3. Gemini Vision: ìŠ¤ìº” ë¬¸ì„œ ì „ìš© (ë¹„ìš© ë°œìƒ)
    """
    logger.info("Parsing PDF with Hybrid Strategy: %s", path)

    try:
        doc = pymupdf.open(path)
        total_text_len = sum(len(page.get_text()) for page in doc)
        is_scanned = (len(doc) > 0) and (total_text_len / len(doc) < 50)
        doc.close()

        if not is_scanned:
            try:
                logger.info("ğŸš€ Docling íŒŒì„œ ì‹œë„ (í‘œ/êµ¬ì¡° ìµœì í™”)")
                parsed_doc = DoclingParser().parse(path)

                # ì™¸ê³¼ì  ë³´ì¶© (Surgical Supplement): Docling ëˆ„ë½ ìˆ˜ì‹ ì±„ìš°ê¸°
                parsed_doc.blocks = _supplement_missing_equations(
                    path, parsed_doc.blocks
                )

                return parsed_doc
            except (ImportError, OSError, RuntimeError) as e:
                logger.warning(
                    "âš ï¸ Docling ì‹¤íŒ¨ (%s). PyMuPDF + SectionClassifierë¡œ ì „í™˜.", e
                )
                return PyMuPDFParser().parse(path)
        else:
            logger.info("ğŸ–¼ï¸ Scanned PDF ê°ì§€: Gemini Vision(VLM) ì‚¬ìš©")
            if not (os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")):
                logger.warning("âš ï¸ Gemini API Key ì—†ìŒ. PyMuPDFë¡œ ê°•ì œ ì§„í–‰")
                return PyMuPDFParser().parse(path)
            return GeminiVisionParser().parse(path)

    except (OSError, RuntimeError, ValueError) as e:
        logger.warning("âš ï¸ íŒŒì‹± ì¤‘ ì—ëŸ¬ (%s). PyMuPDF fallback ëª¨ë“œ.", e)
        return PyMuPDFParser().parse(path)

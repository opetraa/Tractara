# src/clara_ssot/api/pipeline.py
import logging
import os
from pathlib import Path
from typing import Any, Dict

from ..curation.term_curation_service import merge_term_candidates
from ..landing.landing_repository import save_doc_landing, save_term_candidates_landing
from ..normalization.doc_mapper import build_doc_baseline
from ..normalization.term_mapper import (
    build_term_baseline_candidates,
    extract_term_candidates,
)
from ..parsing.pdf_parser import parse_pdf
from ..ssot.doc_ssot_repository import upsert_doc as upsert_doc_ssot
from ..ssot.term_ssot_repository import upsert_terms as upsert_term_ssot
from ..validation.json_schema_validator import schema_registry
from ..validation.term_validator import filter_promotable_terms

logger = logging.getLogger(__name__)


def ingest_single_document(pdf_path: Path) -> Dict[str, Any]:
    """
    PDF 1ê°œì— ëŒ€í•œ ì „ì²´ íŒŒì´í”„ë¼ì¸:
      1) íŒŒì‹±
      2) DOC baseline ìƒì„± + ìŠ¤í‚¤ë§ˆ ê²€ì¦
      3) Landing ì €ì¥ + DOC SSoT ì €ì¥
      4) TERM í›„ë³´ ìƒì„± + Landing ì €ì¥
      5) TERM ë³‘í•© + ìŠ¹ê²© ê°€ëŠ¥ TERM í•„í„°ë§ + TERM SSoT ì €ì¥
      6) ìš”ì•½ ê²°ê³¼ ë°˜í™˜
    """

    # LLM API í‚¤ ê°€ì ¸ì˜¤ê¸°
    llm_api_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")

    warnings = []
    if not llm_api_key:
        msg = "âš ï¸ No LLM API Key found. Term extraction will likely return empty results."
        logger.warning(msg)
        warnings.append(msg)

    # 1) Parsing (ì´ì œ Docling+PyMuPDF ì‚¬ìš©!)
    parsed = parse_pdf(pdf_path)

    # 2) DOC baseline + ìŠ¤í‚¤ë§ˆ ê²€ì¦
    doc_baseline = build_doc_baseline(parsed)
    schema_registry.validate(
        "doc", doc_baseline, instance_path=doc_baseline["documentId"]
    )

    # 3) Landing + DOC SSoT
    doc_id = save_doc_landing(doc_baseline)
    upsert_doc_ssot(doc_baseline)

    # 4) TERM í›„ë³´ ìƒì„± (ì´ì œ LLM ì‚¬ìš©!)
    logger.info("ğŸš€ Starting TERM extraction (after DOC creation)...")
    term_candidates = extract_term_candidates(parsed, llm_api_key=llm_api_key)
    logger.info(f"ğŸ” Extracted {len(term_candidates or [])} term candidates.")

    if not term_candidates and llm_api_key:
        warnings.append("LLM API Key was present, but 0 terms were extracted.")

    term_baseline_candidates = build_term_baseline_candidates(
        doc_id, term_candidates)
    save_term_candidates_landing(doc_id, term_baseline_candidates)

    # 5) TERM ë³‘í•© + ìŠ¹ê²©
    merged_terms = merge_term_candidates(term_baseline_candidates)
    promotable, term_problems = filter_promotable_terms(merged_terms)

    if promotable:
        upsert_term_ssot(promotable)

    # 6) í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ëŒë ¤ì¤„ ê²°ê³¼
    return {
        "documentId": doc_id,
        "promotedTermCount": len(promotable),
        "termValidationProblems": [p.dict() for p in term_problems],
        "warnings": warnings,
    }

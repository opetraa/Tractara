# src/clara_ssot/normalization/term_mapper.py
import re
from dataclasses import dataclass, field
from typing import Any, Dict, List, Tuple
import logging
import os

import instructor
import google.generativeai as genai
from pydantic import BaseModel, Field

from ..models.term_types import TermType
from ..parsing.pdf_parser import ParsedDocument
from ..tracing import get_trace_id

logger = logging.getLogger(__name__)


# Pydantic ëª¨ë¸ë¡œ TERM êµ¬ì¡° ì •ì˜
class ExtractedTerm(BaseModel):
    """LLMì´ ì¶”ì¶œí•  TERM êµ¬ì¡°"""
    term: str = Field(description="ìš©ì–´ (ì•½ì–´ ë˜ëŠ” ì „ì²´ ëª…ì¹­)")
    headword_en: str = Field(description="ì˜ë¬¸ ì •ì‹ ëª…ì¹­")
    headword_ko: str = Field(description="í•œê¸€ ì •ì‹ ëª…ì¹­")
    definition_en: str = Field(description="ì˜ë¬¸ ì •ì˜")
    definition_ko: str = Field(description="í•œê¸€ ì •ì˜")
    domain: List[str] = Field(description="ë„ë©”ì¸ íƒœê·¸", default=["nuclear"])
    context: str = Field(description="ì›ë¬¸ ë§¥ë½ (ì¶œì²˜ ë¬¸ì¥)")
    term_type: TermType = Field(
        description="TERM íƒ€ì… (í˜„ì¬ ë‹¨ê³„: ì¼ë°˜ ë¬¸ì„œ ì¶”ì¶œì€ ëª¨ë‘ TERM-CLASSë¡œ ê³ ì •)",
        default=TermType.CLASS,
    )


class TermExtractionResult(BaseModel):
    """LLM ì‘ë‹µ ì „ì²´ êµ¬ì¡°"""
    terms: List[ExtractedTerm] = Field(description="ì¶”ì¶œëœ ìš©ì–´ ëª©ë¡")
    reasoning: str = Field(description="Chain of Thought ì¶”ë¡  ê³¼ì •")


@dataclass
class TermCandidate:
    term: str
    definition_en: str | None = None
    definition_ko: str | None = None
    headword_en: str | None = None
    headword_ko: str | None = None
    domain: List[str] | None = None
    context: str | None = None
    term_type: TermType = field(default=TermType.CLASS)


class LLMTermExtractor:
    """
    LLM ê¸°ë°˜ TERM ì¶”ì¶œê¸° (ì²« ë²ˆì§¸ ë¬¸ì„œ ì „ëµ: CoT + Pydantic)
    """

    def __init__(self, api_key: str):
        # Gemini ì„¤ì • (êµ¬í˜• SDK ì‚¬ìš© - ì•ˆì •ì„± í™•ë³´)
        # TODO: [Migration] instructor ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ google-genai(ì‹ í˜• SDK)ë¥¼ ì™„ë²½íˆ ì§€ì›í•˜ë©´ ë§ˆì´ê·¸ë ˆì´ì…˜ í•„ìš”.
        # í˜„ì¬(2026.02) instructor 1.14.x ë²„ì „ì€ êµ¬í˜• SDK(google-generativeai)ì™€ í˜¸í™˜ì„±ì´ ë” ì¢‹ìŒ.
        # ì°¸ì¡°: https://github.com/google-gemini/deprecated-generative-ai-python
        genai.configure(api_key=api_key)

        # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ í›„ë³´êµ° ì„¤ì • (Fallbackì„ ìœ„í•´ ë¦¬ìŠ¤íŠ¸ë¡œ ê´€ë¦¬)
        self.model_candidates = self._get_model_candidates()
        self.current_model_idx = 0
        self.model_name = self.model_candidates[0]

        logger.info(f"ğŸ¤– Initializing Gemini with model: {self.model_name}")
        self._init_client()

    def _init_client(self):
        # Instructor í´ë¼ì´ì–¸íŠ¸ ë˜í•‘ (í‘œì¤€í™”ëœ ì¸í„°í˜ì´ìŠ¤ ì œê³µ)
        # êµ¬í˜• SDKì˜ GenerativeModel ê°ì²´ë¥¼ ìƒì„±í•˜ì—¬ ì „ë‹¬
        self.client = instructor.from_gemini(
            client=genai.GenerativeModel(model_name=self.model_name),
            mode=instructor.Mode.GEMINI_JSON,
        )

    def _get_model_candidates(self) -> List[str]:
        """API í‚¤ë¡œ ì ‘ê·¼ ê°€ëŠ¥í•œ ëª¨ë¸ ì¤‘ ìµœì ì˜ ëª¨ë¸ í›„ë³´ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
        candidates = []
        target_model = os.getenv("GEMINI_MODEL")

        # 1. í™˜ê²½ë³€ìˆ˜ ëª¨ë¸ ìµœìš°ì„ 
        if target_model:
            candidates.append(target_model)

        # 2. ì„ í˜¸í•˜ëŠ” ëª¨ë¸ ìˆœì„œ (ì„±ëŠ¥/ë¹„ìš©/ì¿¼í„° ê³ ë ¤)
        # 429 ì—ëŸ¬ ë°œìƒ ì‹œ ìˆœì°¨ì ìœ¼ë¡œ ë‹¤ìŒ ëª¨ë¸ì„ ì‹œë„í•¨
        preferences = [
            "gemini-3-flash-preview",
            "gemini-2.5-pro",
            "gemini-2.5-flash",
            "gemini-2.5-flash-lite",
            "gemini-2.0-flash",
            "gemini-1.5-flash",
            "gemini-1.5-flash-8b",
            "gemini-1.5-pro",
        ]

        try:
            # 3. ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ
            available_models = [m.name.replace(
                "models/", "") for m in genai.list_models()]
            logger.info(f"ğŸ“‹ Available Gemini models: {available_models}")

            for pref in preferences:
                if pref in available_models and pref not in candidates:
                    candidates.append(pref)

            # 4. ì„ í˜¸ ëª¨ë¸ì´ ì—†ìœ¼ë©´ ëª©ë¡ì˜ ì²« ë²ˆì§¸ ëª¨ë¸ ì‚¬ìš©
            if not candidates and available_models:
                candidates.append(available_models[0])

        except Exception as e:
            logger.error(f"âš ï¸ Failed to list models: {e}")
            # API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë¦¬ìŠ¤íŠ¸ ì‚¬ìš©
            for pref in preferences:
                if pref not in candidates:
                    candidates.append(pref)

        if not candidates:
            candidates = ["gemini-1.5-flash"]

        return candidates

    def extract(self, text_chunks: List[str]) -> Tuple[List[TermCandidate], List[str]]:
        """
        ì—¬ëŸ¬ í…ìŠ¤íŠ¸ ì²­í¬ì—ì„œ TERM ì¶”ì¶œ
        Returns: (candidates, error_messages)
        """
        all_candidates = []
        errors = []

        for i, chunk in enumerate(text_chunks):
            if len(chunk.strip()) < 20:  # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ëŠ” ìŠ¤í‚µ (ê¸°ì¤€ ì™„í™”: 50 -> 20)
                continue

            logger.info(
                f"Sending chunk {i+1}/{len(text_chunks)} to LLM (len={len(chunk)})...")
            try:
                result = self._extract_from_chunk(chunk)
                candidates = [
                    TermCandidate(
                        term=t.term,
                        definition_en=t.definition_en,
                        definition_ko=t.definition_ko,
                        headword_en=t.headword_en,
                        headword_ko=t.headword_ko,
                        domain=t.domain,
                        context=t.context,
                        term_type=t.term_type,
                    )
                    for t in result.terms
                ]
                all_candidates.extend(candidates)

                logger.info(f"Extracted {len(candidates)} terms from chunk")
                logger.debug(f"CoT reasoning: {result.reasoning}")

            except Exception as e:
                msg = f"Chunk {i+1} failed: {str(e)}"
                logger.error(f"âŒ TERM extraction failed: {msg}", exc_info=True)

                # ğŸš¨ API í‚¤ ë§Œë£Œ ë˜ëŠ” ê¶Œí•œ ì—ëŸ¬ ë°œìƒ ì‹œ ì¦‰ì‹œ ì¤‘ë‹¨
                if "expired" in str(e).lower() or "400" in str(e) or "403" in str(e):
                    logger.critical(
                        "ğŸ›‘ Critical API Error: API Key expired or invalid. Stopping.")
                    errors.append(
                        "Critical: API Key issue. Please check your .env file.")
                    break

                # 404 ëª¨ë¸ ì—ëŸ¬ì¸ ê²½ìš° ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
                if "404" in str(e) and "models/" in str(e):
                    try:
                        available_models = [
                            m.name for m in genai.list_models()]
                        logger.error(f"Available models: {available_models}")
                    except Exception as list_err:
                        logger.error(f"Failed to list models: {list_err}")

                errors.append(msg)

        return all_candidates, errors

    def _extract_from_chunk(self, text: str) -> TermExtractionResult:
        """
        ë‹¨ì¼ ì²­í¬ì—ì„œ TERM ì¶”ì¶œ (Instructor + CoT)
        """
        # Few-shot ì˜ˆì œ
        few_shot_example = """
ì˜ˆì‹œ 1:
í…ìŠ¤íŠ¸: "ê²½ë…„ì—´í™” ê´€ë¦¬ í”„ë¡œê·¸ë¨(AMP)ì€ ì›ìë ¥ ë°œì „ì†Œì˜ ì¥ê¸° ìš´ì „ì„ ìœ„í•´ í•„ìˆ˜ì ì´ë‹¤. ë˜í•œ 1ì°¨ ê³„í†µì˜ ì‘ë ¥ë¶€ì‹ê· ì—´(SCC) ë° í”¼ë¡œ(Fatigue) ì†ìƒì„ ê°ì‹œí•´ì•¼ í•œë‹¤."

ì¶”ì¶œ:
[
  {
    "term": "AMP",
    "headword_en": "Aging Management Program",
    "headword_ko": "ê²½ë…„ì—´í™” ê´€ë¦¬ í”„ë¡œê·¸ë¨",
    "definition_en": "A program to manage aging effects in nuclear power plants.",
    "definition_ko": "ì›ìë ¥ ë°œì „ì†Œì˜ ê²½ë…„ ì—´í™” ì˜í–¥ì„ ê´€ë¦¬í•˜ëŠ” í”„ë¡œê·¸ë¨.",
    "domain": ["nuclear", "LTO", "safety"],
    "context": "ê²½ë…„ì—´í™” ê´€ë¦¬ í”„ë¡œê·¸ë¨(AMP)ì€..."
  },
  {
    "term": "SCC",
    "headword_en": "Stress Corrosion Cracking",
    "headword_ko": "ì‘ë ¥ë¶€ì‹ê· ì—´",
    "definition_en": "Cracking induced from the combined influence of tensile stress and a corrosive environment.",
    "definition_ko": "ì¸ì¥ ì‘ë ¥ê³¼ ë¶€ì‹ì„± í™˜ê²½ì˜ ë³µí•©ì ì¸ ì˜í–¥ìœ¼ë¡œ ë°œìƒí•˜ëŠ” ê· ì—´.",
    "domain": ["nuclear", "materials"],
    "context": "ë˜í•œ 1ì°¨ ê³„í†µì˜ ì‘ë ¥ë¶€ì‹ê· ì—´(SCC) ë°..."
  },
  {
    "term": "Fatigue",
    "headword_en": "Fatigue",
    "headword_ko": "í”¼ë¡œ",
    "definition_en": "Weakening of a material caused by cyclic loading.",
    "definition_ko": "ë°˜ë³µì ì¸ í•˜ì¤‘ìœ¼ë¡œ ì¸í•´ ì¬ë£Œê°€ ì•½í•´ì§€ëŠ” í˜„ìƒ.",
    "domain": ["nuclear", "materials", "mechanics"],
    "context": "...ë° í”¼ë¡œ(Fatigue) ì†ìƒì„ ê°ì‹œí•´ì•¼ í•œë‹¤."
  }
]
"""

        prompt = f"""ë‹¹ì‹ ì€ ê¸°ìˆ  ë¬¸ì„œ(ì›ìë ¥, ì—”ì§€ë‹ˆì–´ë§, IT, í™˜ê²½ ë“±)ì—ì„œ ì „ë¬¸ ìš©ì–´ë¥¼ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë¬¸ì„œì— ë“±ì¥í•˜ëŠ” 'ëª¨ë“ ' ì£¼ìš” ê¸°ìˆ  ìš©ì–´, ì•½ì–´, ì‹œìŠ¤í…œ ëª…ì¹­, ê³ ìœ í•œ ê°œë…ì„ ì¶”ì¶œí•˜ì„¸ìš”.
ë‹¨ìˆœíˆ íŠ¹ì • ë‹¨ì–´ë§Œ ì°¾ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ë¬¸ì„œ ë‚´ì˜ ë‹¤ì–‘í•œ ì „ë¬¸ ìš©ì–´ë¥¼ í¬ê´„ì ìœ¼ë¡œ ì°¾ì•„ì•¼ í•©ë‹ˆë‹¤.

ë‹¤ìŒ ë‹¨ê³„ë¥¼ ë”°ë¥´ì„¸ìš”:
1. í…ìŠ¤íŠ¸ë¥¼ ì •ë…í•˜ê³  ì•½ì–´(ì˜ˆ: LOCA, RPV), í•œê¸€ ì „ë¬¸ ìš©ì–´(ì˜ˆ: ëƒ‰ê°ì¬ìƒì‹¤ì‚¬ê³ ), ì˜ë¬¸ ì „ë¬¸ ìš©ì–´(ì˜ˆ: Fatigue Monitoring)ë¥¼ ëª¨ë‘ ì‹ë³„í•˜ì„¸ìš”.
2. ê° ìš©ì–´ì˜ ì •ì‹ ëª…ì¹­(Full Name)ì„ ì˜ë¬¸ê³¼ í•œê¸€ë¡œ ìµœëŒ€í•œ ë³µì›í•˜ì„¸ìš”.
3. ë¬¸ë§¥(Context)ì„ ë°”íƒ•ìœ¼ë¡œ í•´ë‹¹ ìš©ì–´ì˜ ì •ì˜ë¥¼ ìš”ì•½í•˜ì„¸ìš”.
4. ì ì ˆí•œ ë„ë©”ì¸ íƒœê·¸ë¥¼ í• ë‹¹í•˜ì„¸ìš” (nuclear, safety, LTO, PSR, FSAR ë“±).

{few_shot_example}

ì´ì œ ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ìš©ì–´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”:

{text}

ì¶”ë¡  ê³¼ì •ì„ reasoning í•„ë“œì— ìì„¸íˆ ê¸°ë¡í•˜ê³ , terms ë°°ì—´ì— ì¶”ì¶œ ê²°ê³¼ë¥¼ ë‹´ìœ¼ì„¸ìš”.
"""

        # ëª¨ë¸ Fallback ë£¨í”„
        while True:
            try:
                # Instructorë¥¼ í†µí•œ êµ¬ì¡°í™”ëœ ì¶œë ¥ ìš”ì²­
                response = self.client.chat.completions.create(
                    messages=[{"role": "user", "content": prompt}],
                    response_model=TermExtractionResult,
                    max_retries=2,  # ë‚´ë¶€ ì¬ì‹œë„ (ì¼ì‹œì  ì˜¤ë¥˜ìš©)
                )
                return response
            except Exception as e:
                # 429 Quota Exceeded ì—ëŸ¬ ì²˜ë¦¬
                if "429" in str(e) or "Quota exceeded" in str(e) or "ResourceExhausted" in str(e):
                    logger.warning(
                        f"âš ï¸ Quota exceeded for model {self.model_name}.")

                    # ë‹¤ìŒ ëª¨ë¸ë¡œ ì „í™˜
                    self.current_model_idx += 1
                    if self.current_model_idx < len(self.model_candidates):
                        self.model_name = self.model_candidates[self.current_model_idx]
                        logger.info(
                            f"ğŸ”„ Switching to fallback model: {self.model_name}")
                        self._init_client()
                        continue
                    else:
                        logger.error("âŒ All fallback models exhausted.")
                        raise e
                raise e


def extract_term_candidates(
    parsed: ParsedDocument,
    llm_api_key: str = None
) -> Tuple[List[TermCandidate], List[str]]:
    """
    ParsedDocumentì—ì„œ TERM í›„ë³´ ì¶”ì¶œ

    ì²« ë²ˆì§¸ ë¬¸ì„œ ì „ëµ:
    - LLM ê¸°ë°˜ ì¶”ì¶œ (CoT + Few-shot)
    - Instructorë¡œ êµ¬ì¡°í™”ëœ ì¶œë ¥ ë³´ì¥
    """
    # ì¸ìë¡œ í‚¤ê°€ ì•ˆ ë„˜ì–´ì™”ìœ¼ë©´ í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì¡°íšŒ
    if not llm_api_key:
        llm_api_key = os.environ.get(
            "GEMINI_API_KEY") or os.environ.get("GOOGLE_API_KEY")

    if not llm_api_key:
        logger.warning("No LLM API key provided, using dummy extractor")
        return [
            TermCandidate(
                term="AMP",
                definition_en=None,
                definition_ko="ê²½ë…„ì—´í™” ê´€ë¦¬ í”„ë¡œê·¸ë¨",
            )
        ], ["No LLM API Key provided."]

    # í…ìŠ¤íŠ¸ ì²­í¬ ì¤€ë¹„
    text_chunks = [
        block.text for block in parsed.blocks
        if block.text and len(block.text) > 20
    ]

    if not text_chunks:
        logger.warning(
            "âš ï¸ No text chunks > 20 chars found in document. Term extraction skipped.")
        return [], ["No text chunks found in document (OCR might be needed)."]

    # LLM ì¶”ì¶œ
    extractor = LLMTermExtractor(api_key=llm_api_key)
    # ë” ë§ì€ ìš©ì–´ë¥¼ ì°¾ê¸° ìœ„í•´ ì²­í¬ ìˆ˜ ì¦ê°€ (í…ŒìŠ¤íŠ¸ìš© 1ê°œ)
    chunks_to_process = text_chunks[:1]
    logger.info(f"Sending {len(chunks_to_process)} text chunks to LLM...")
    candidates, errors = extractor.extract(chunks_to_process)

    logger.info(
        f"Extracted {len(candidates)} TERM candidates. Errors: {len(errors)}")
    return candidates, errors


def _normalize_term_id(headword_en: str | None, fallback: str, term_type: TermType) -> str:
    """
    ê°•íƒ€ì… URN í˜•ì‹ì˜ termIdë¥¼ ìƒì„±í•œë‹¤.

    ìš°ì„ ìˆœìœ„: headword_en > fallback(term í•„ë“œ)
    ì •ê·œí™” ê·œì¹™:
    - ì˜ë¬¸ ì†Œë¬¸ìë¡œ ë³€í™˜
    - ê³µë°±/íŠ¹ìˆ˜ë¬¸ì â†’ ì–¸ë”ìŠ¤ì½”ì–´
    - ASCII ì˜ìˆ«ì + ì–¸ë”ìŠ¤ì½”ì–´ë§Œ í—ˆìš© (í•œê¸€ ë“± ë¹„ASCII ì œê±°)
    - ë¹ˆ ë¬¸ìì—´ì´ ë˜ë©´ "unknown"ìœ¼ë¡œ ëŒ€ì²´
    """
    prefix_map = {
        TermType.CLASS: "term:class:",
        TermType.REL: "term:rel:",
        TermType.RULE: "term:rule:",
    }
    prefix = prefix_map[term_type]

    source = (headword_en or "").strip() or fallback.strip()

    # ASCII ì˜ìˆ«ìì™€ ê³µë°±ë§Œ ë‚¨ê¸°ê³  ì œê±°
    ascii_only = re.sub(r"[^a-zA-Z0-9 ]", " ", source)
    # ì†Œë¬¸ì ë³€í™˜ + ê³µë°± ì—°ì† â†’ ë‹¨ì¼ ì–¸ë”ìŠ¤ì½”ì–´
    normalized = re.sub(r"\s+", "_", ascii_only.lower().strip())
    # ì„ í–‰/í›„í–‰ ì–¸ë”ìŠ¤ì½”ì–´ ì œê±°
    normalized = normalized.strip("_")

    if not normalized:
        normalized = "unknown"

    return prefix + normalized


def build_term_baseline_candidates(
    doc_baseline_id: str,
    candidates: List[TermCandidate],
) -> List[Dict[str, Any]]:
    """
    TermCandidate â†’ TERM Baseline JSON ë³€í™˜.

    termIdëŠ” headword_en ê¸°ë°˜ ê°•íƒ€ì… URNìœ¼ë¡œ ìƒì„±í•œë‹¤:
    - TERM-CLASS: term:class:{normalized_headword_en}
    - TERM-REL:   term:rel:{normalized_headword_en}
    - TERM-RULE:  term:rule:{normalized_headword_en}
    """
    term_jsons: List[Dict[str, Any]] = []

    for c in candidates:
        term_id = _normalize_term_id(c.headword_en, c.term, c.term_type)
        term_json = {
            "type": "term_entry",
            "termId": term_id,
            "termType": c.term_type.value,
            "term": c.term,
            "lang": "bilingual",
            "headword_en": c.headword_en or c.term,
            "headword_ko": c.headword_ko or "",
            "definition_en": c.definition_en or "[PENDING_DEFINITION]",
            "definition_ko": c.definition_ko or "",
            "slots": {
                "context_ko": c.context or "",
            },
            "examples": [],
            "negatives": [],
            "domain": c.domain or ["nuclear"],
            "relatedTerms": [],
            "relations": [],
            "taxonomyBindings": [],
            "provenance": {
                "sources": [{"docId": doc_baseline_id}],
                "curationStatus": "candidate",
                "trace_id": get_trace_id(),
            },
            "status": "candidate",
        }
        term_jsons.append(term_json)

    return term_jsons

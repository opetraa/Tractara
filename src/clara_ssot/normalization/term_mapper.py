# src/clara_ssot/normalization/term_mapper.py
from dataclasses import dataclass
from typing import Any, Dict, List, Tuple
import logging
import os
import uuid

import instructor
import google.generativeai as genai
from pydantic import BaseModel, Field

from ..parsing.pdf_parser import ParsedDocument
from ..tracing import get_trace_id

logger = logging.getLogger(__name__)


# Pydantic ëª¨ë¸ë¡œ TERM êµ¬ì¡° ì •ì˜
class ExtractedTerm(BaseModel):
    """LLMì´ ì¶”ì¶œí•  TERM êµ¬ì¡°"""
    term: str = Field(description="ìš©ì–´ (ì•½ì–´ ë˜ëŠ” ì „ì²´ ëª…ì¹­)")
    headword_en: str = Field(description="ì˜ë¬¸ ì •ì‹ ëª…ì¹­")
    headword_ko: str = Field(description="í•œê¸€ ì •ì‹ ëª…ì¹­")
    definition_en: str = Field(description="ì˜ë¬¸ ì •ì˜")
    definition_ko: str = Field(description="í•œê¸€ ì •ì˜")
    domain: List[str] = Field(description="ë„ë©”ì¸ íƒœê·¸", default=["nuclear"])
    context: str = Field(description="ì›ë¬¸ ë§¥ë½ (ì¶œì²˜ ë¬¸ì¥)")


class TermExtractionResult(BaseModel):
    """LLM ì‘ë‹µ ì „ì²´ êµ¬ì¡°"""
    terms: List[ExtractedTerm] = Field(description="ì¶”ì¶œëœ ìš©ì–´ ëª©ë¡")
    reasoning: str = Field(description="Chain of Thought ì¶”ë¡  ê³¼ì •")


@dataclass
class TermCandidate:
    term: str
    definition_en: str | None = None
    definition_ko: str | None = None
    headword_en: str | None = None
    headword_ko: str | None = None
    domain: List[str] | None = None
    context: str | None = None


class LLMTermExtractor:
    """
    LLM ê¸°ë°˜ TERM ì¶”ì¶œê¸° (ì²« ë²ˆì§¸ ë¬¸ì„œ ì „ëµ: CoT + Pydantic)
    """

    def __init__(self, api_key: str):
        # Gemini ì„¤ì •
        genai.configure(api_key=api_key)

        # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì¤‘ ìµœì ì˜ ëª¨ë¸ ìë™ ì„ íƒ
        model_name = self._select_best_model()
        logger.info(f"ğŸ¤– Initializing Gemini with model: {model_name}")

        self.client = instructor.from_gemini(
            client=genai.GenerativeModel(model_name=model_name),
            mode=instructor.Mode.GEMINI_JSON,
        )

    def _select_best_model(self) -> str:
        """API í‚¤ë¡œ ì ‘ê·¼ ê°€ëŠ¥í•œ ëª¨ë¸ ì¤‘ ìµœì ì˜ ëª¨ë¸ì„ ìë™ìœ¼ë¡œ ì„ íƒ"""
        target_model = os.getenv("GEMINI_MODEL")

        try:
            # 1. ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ
            all_models = list(genai.list_models())
            # generateContent ê¸°ëŠ¥ì„ ì§€ì›í•˜ëŠ” ëª¨ë¸ë§Œ í•„í„°ë§
            available_models = [
                m.name.replace("models/", "")
                for m in all_models
                if "generateContent" in m.supported_generation_methods
            ]

            logger.info(f"ğŸ“‹ Available Gemini models: {available_models}")

            # 2. í™˜ê²½ë³€ìˆ˜ë¡œ ì§€ì •ëœ ëª¨ë¸ì´ ìœ íš¨í•œì§€ í™•ì¸
            if target_model:
                if target_model in available_models:
                    return target_model
                logger.warning(
                    f"âš ï¸ Configured model '{target_model}' not found. Attempting auto-selection.")

            # 3. ì„ í˜¸í•˜ëŠ” ëª¨ë¸ ìˆœì„œëŒ€ë¡œ í™•ì¸ (ìµœì‹  ë²„ì „ ìš°ì„ )
            preferences = [
                "gemini-1.5-flash-002",
                "gemini-1.5-flash-001",
                "gemini-1.5-flash",
                "gemini-1.5-pro-002",
                "gemini-1.5-pro-001",
                "gemini-1.5-pro",
            ]

            for pref in preferences:
                if pref in available_models:
                    return pref

            # 4. ì„ í˜¸ ëª¨ë¸ì´ ì—†ìœ¼ë©´ ëª©ë¡ì˜ ì²« ë²ˆì§¸ ëª¨ë¸ ì‚¬ìš©
            if available_models:
                return available_models[0]

        except Exception as e:
            logger.error(f"âš ï¸ Failed to list models: {e}")

        # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
        return target_model or "gemini-1.5-flash"

    def extract(self, text_chunks: List[str]) -> Tuple[List[TermCandidate], List[str]]:
        """
        ì—¬ëŸ¬ í…ìŠ¤íŠ¸ ì²­í¬ì—ì„œ TERM ì¶”ì¶œ
        Returns: (candidates, error_messages)
        """
        all_candidates = []
        errors = []

        for i, chunk in enumerate(text_chunks):
            if len(chunk.strip()) < 20:  # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ëŠ” ìŠ¤í‚µ (ê¸°ì¤€ ì™„í™”: 50 -> 20)
                continue

            logger.info(
                f"Sending chunk {i+1}/{len(text_chunks)} to LLM (len={len(chunk)})...")
            try:
                result = self._extract_from_chunk(chunk)
                candidates = [
                    TermCandidate(
                        term=t.term,
                        definition_en=t.definition_en,
                        definition_ko=t.definition_ko,
                        headword_en=t.headword_en,
                        headword_ko=t.headword_ko,
                        domain=t.domain,
                        context=t.context
                    )
                    for t in result.terms
                ]
                all_candidates.extend(candidates)

                logger.info(f"Extracted {len(candidates)} terms from chunk")
                logger.debug(f"CoT reasoning: {result.reasoning}")

            except Exception as e:
                msg = f"Chunk {i+1} failed: {str(e)}"
                logger.error(f"âŒ TERM extraction failed: {msg}", exc_info=True)

                # 404 ëª¨ë¸ ì—ëŸ¬ì¸ ê²½ìš° ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
                if "404" in str(e) and "models/" in str(e):
                    try:
                        available_models = [
                            m.name for m in genai.list_models()]
                        logger.error(f"Available models: {available_models}")
                    except Exception as list_err:
                        logger.error(f"Failed to list models: {list_err}")

                errors.append(msg)

        return all_candidates, errors

    def _extract_from_chunk(self, text: str) -> TermExtractionResult:
        """
        ë‹¨ì¼ ì²­í¬ì—ì„œ TERM ì¶”ì¶œ (Instructor + CoT)
        """
        # Few-shot ì˜ˆì œ
        few_shot_example = """
ì˜ˆì‹œ 1:
í…ìŠ¤íŠ¸: "ê²½ë…„ì—´í™” ê´€ë¦¬ í”„ë¡œê·¸ë¨(AMP)ì€ ì›ìë ¥ ë°œì „ì†Œì˜ ì¥ê¸° ìš´ì „ì„ ìœ„í•´ í•„ìˆ˜ì ì´ë‹¤. ë˜í•œ 1ì°¨ ê³„í†µì˜ ì‘ë ¥ë¶€ì‹ê· ì—´(SCC) ë° í”¼ë¡œ(Fatigue) ì†ìƒì„ ê°ì‹œí•´ì•¼ í•œë‹¤."

ì¶”ì¶œ:
[
  {
    "term": "AMP",
    "headword_en": "Aging Management Program",
    "headword_ko": "ê²½ë…„ì—´í™” ê´€ë¦¬ í”„ë¡œê·¸ë¨",
    "definition_en": "A program to manage aging effects in nuclear power plants.",
    "definition_ko": "ì›ìë ¥ ë°œì „ì†Œì˜ ê²½ë…„ ì—´í™” ì˜í–¥ì„ ê´€ë¦¬í•˜ëŠ” í”„ë¡œê·¸ë¨.",
    "domain": ["nuclear", "LTO", "safety"],
    "context": "ê²½ë…„ì—´í™” ê´€ë¦¬ í”„ë¡œê·¸ë¨(AMP)ì€..."
  },
  {
    "term": "SCC",
    "headword_en": "Stress Corrosion Cracking",
    "headword_ko": "ì‘ë ¥ë¶€ì‹ê· ì—´",
    "definition_en": "Cracking induced from the combined influence of tensile stress and a corrosive environment.",
    "definition_ko": "ì¸ì¥ ì‘ë ¥ê³¼ ë¶€ì‹ì„± í™˜ê²½ì˜ ë³µí•©ì ì¸ ì˜í–¥ìœ¼ë¡œ ë°œìƒí•˜ëŠ” ê· ì—´.",
    "domain": ["nuclear", "materials"],
    "context": "ë˜í•œ 1ì°¨ ê³„í†µì˜ ì‘ë ¥ë¶€ì‹ê· ì—´(SCC) ë°..."
  },
  {
    "term": "Fatigue",
    "headword_en": "Fatigue",
    "headword_ko": "í”¼ë¡œ",
    "definition_en": "Weakening of a material caused by cyclic loading.",
    "definition_ko": "ë°˜ë³µì ì¸ í•˜ì¤‘ìœ¼ë¡œ ì¸í•´ ì¬ë£Œê°€ ì•½í•´ì§€ëŠ” í˜„ìƒ.",
    "domain": ["nuclear", "materials", "mechanics"],
    "context": "...ë° í”¼ë¡œ(Fatigue) ì†ìƒì„ ê°ì‹œí•´ì•¼ í•œë‹¤."
  }
]
"""

        prompt = f"""ë‹¹ì‹ ì€ ê¸°ìˆ  ë¬¸ì„œ(ì›ìë ¥, ì—”ì§€ë‹ˆì–´ë§, IT, í™˜ê²½ ë“±)ì—ì„œ ì „ë¬¸ ìš©ì–´ë¥¼ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë¬¸ì„œì— ë“±ì¥í•˜ëŠ” 'ëª¨ë“ ' ì£¼ìš” ê¸°ìˆ  ìš©ì–´, ì•½ì–´, ì‹œìŠ¤í…œ ëª…ì¹­, ê³ ìœ í•œ ê°œë…ì„ ì¶”ì¶œí•˜ì„¸ìš”.
ë‹¨ìˆœíˆ íŠ¹ì • ë‹¨ì–´ë§Œ ì°¾ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ë¬¸ì„œ ë‚´ì˜ ë‹¤ì–‘í•œ ì „ë¬¸ ìš©ì–´ë¥¼ í¬ê´„ì ìœ¼ë¡œ ì°¾ì•„ì•¼ í•©ë‹ˆë‹¤.

ë‹¤ìŒ ë‹¨ê³„ë¥¼ ë”°ë¥´ì„¸ìš”:
1. í…ìŠ¤íŠ¸ë¥¼ ì •ë…í•˜ê³  ì•½ì–´(ì˜ˆ: LOCA, RPV), í•œê¸€ ì „ë¬¸ ìš©ì–´(ì˜ˆ: ëƒ‰ê°ì¬ìƒì‹¤ì‚¬ê³ ), ì˜ë¬¸ ì „ë¬¸ ìš©ì–´(ì˜ˆ: Fatigue Monitoring)ë¥¼ ëª¨ë‘ ì‹ë³„í•˜ì„¸ìš”.
2. ê° ìš©ì–´ì˜ ì •ì‹ ëª…ì¹­(Full Name)ì„ ì˜ë¬¸ê³¼ í•œê¸€ë¡œ ìµœëŒ€í•œ ë³µì›í•˜ì„¸ìš”.
3. ë¬¸ë§¥(Context)ì„ ë°”íƒ•ìœ¼ë¡œ í•´ë‹¹ ìš©ì–´ì˜ ì •ì˜ë¥¼ ìš”ì•½í•˜ì„¸ìš”.
4. ì ì ˆí•œ ë„ë©”ì¸ íƒœê·¸ë¥¼ í• ë‹¹í•˜ì„¸ìš” (nuclear, safety, LTO, PSR, FSAR ë“±).

{few_shot_example}

ì´ì œ ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ìš©ì–´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”:

{text}

ì¶”ë¡  ê³¼ì •ì„ reasoning í•„ë“œì— ìì„¸íˆ ê¸°ë¡í•˜ê³ , terms ë°°ì—´ì— ì¶”ì¶œ ê²°ê³¼ë¥¼ ë‹´ìœ¼ì„¸ìš”.
"""

        # Instructorë¡œ êµ¬ì¡°í™”ëœ ì¶œë ¥ ê°•ì œ
        response = self.client.chat.completions.create(
            messages=[
                {"role": "user", "content": prompt}
            ],
            response_model=TermExtractionResult,
            max_retries=3,
        )

        return response


def extract_term_candidates(
    parsed: ParsedDocument,
    llm_api_key: str = None
) -> Tuple[List[TermCandidate], List[str]]:
    """
    ParsedDocumentì—ì„œ TERM í›„ë³´ ì¶”ì¶œ

    ì²« ë²ˆì§¸ ë¬¸ì„œ ì „ëµ:
    - LLM ê¸°ë°˜ ì¶”ì¶œ (CoT + Few-shot)
    - Instructorë¡œ êµ¬ì¡°í™”ëœ ì¶œë ¥ ë³´ì¥
    """
    # ì¸ìë¡œ í‚¤ê°€ ì•ˆ ë„˜ì–´ì™”ìœ¼ë©´ í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì¡°íšŒ
    if not llm_api_key:
        llm_api_key = os.environ.get(
            "GEMINI_API_KEY") or os.environ.get("GOOGLE_API_KEY")

    if not llm_api_key:
        logger.warning("No LLM API key provided, using dummy extractor")
        return [
            TermCandidate(
                term="AMP",
                definition_en=None,
                definition_ko="ê²½ë…„ì—´í™” ê´€ë¦¬ í”„ë¡œê·¸ë¨",
            )
        ], ["No LLM API Key provided."]

    # í…ìŠ¤íŠ¸ ì²­í¬ ì¤€ë¹„
    text_chunks = [
        block.text for block in parsed.blocks
        if block.text and len(block.text) > 20
    ]

    if not text_chunks:
        logger.warning(
            "âš ï¸ No text chunks > 20 chars found in document. Term extraction skipped.")
        return [], ["No text chunks found in document (OCR might be needed)."]

    # LLM ì¶”ì¶œ
    extractor = LLMTermExtractor(api_key=llm_api_key)
    # ë” ë§ì€ ìš©ì–´ë¥¼ ì°¾ê¸° ìœ„í•´ ì²­í¬ ìˆ˜ ì¦ê°€ (í…ŒìŠ¤íŠ¸ìš© 10ê°œ)
    chunks_to_process = text_chunks[:10]
    logger.info(f"Sending {len(chunks_to_process)} text chunks to LLM...")
    candidates, errors = extractor.extract(chunks_to_process)

    logger.info(
        f"Extracted {len(candidates)} TERM candidates. Errors: {len(errors)}")
    return candidates, errors


def build_term_baseline_candidates(
    doc_baseline_id: str,
    candidates: List[TermCandidate],
) -> List[Dict[str, Any]]:
    """
    TermCandidate â†’ TERM Baseline JSON ë³€í™˜
    """
    term_jsons: List[Dict[str, Any]] = []

    for c in candidates:
        term_id = str(uuid.uuid4())
        term_json = {
            "type": "term_entry",
            "termId": term_id,
            "term": c.term,
            "lang": "bilingual",
            "headword_en": c.headword_en or c.term,
            "headword_ko": c.headword_ko or "",
            "definition_en": c.definition_en or "[PENDING_DEFINITION]",
            "definition_ko": c.definition_ko or "",
            "slots": {
                "context_ko": c.context or "",
            },
            "examples": [],
            "negatives": [],
            "domain": c.domain or ["nuclear"],
            "relatedTerms": [],
            "relations": [],
            "taxonomyBindings": [],
            "provenance": {
                "sources": [{"docId": doc_baseline_id}],
                "curationStatus": "candidate",
                "trace_id": get_trace_id(),
            },
            "status": "candidate",
        }
        term_jsons.append(term_json)

    return term_jsons
